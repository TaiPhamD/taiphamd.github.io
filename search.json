[
  {
    "objectID": "pr.html",
    "href": "pr.html",
    "title": "My Contributions",
    "section": "",
    "text": "App to allow Siri/Google integration with Huyndai’s BlueLink\nApp to allow Siri/Google to control UEFI Boot OS entry, and Wake/Suspend/Shutdown PC"
  },
  {
    "objectID": "pr.html#personal-repos",
    "href": "pr.html#personal-repos",
    "title": "My Contributions",
    "section": "",
    "text": "App to allow Siri/Google integration with Huyndai’s BlueLink\nApp to allow Siri/Google to control UEFI Boot OS entry, and Wake/Suspend/Shutdown PC"
  },
  {
    "objectID": "pr.html#pull-requests",
    "href": "pr.html#pull-requests",
    "title": "My Contributions",
    "section": "Pull Requests",
    "text": "Pull Requests\n\nPyTorch: Adding support for torch.erfinv function for MPS devices . (This PR is a work in progress)\nPyTorch: fixing a bug in ONNX reduction ops during export. (Merged)\nOpencore Bootloader: Add GUI timeout to auto select UEFI OS boot entry(Merged)\nEthminer: Decouple CL and CUDA device context(Merged)\nEthminer: Fix CUDA device initialization bug(Merged)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "taiphamd",
    "section": "",
    "text": "Mounting NAS using autofs\n\n\n\nNAS\n\nnfs\n\ncifs\n\nautofs\n\n\n\n\n\n\n\n\n\nFeb 8, 2026\n\n\nPeter Pham\n\n\n\n\n\n\n\n\n\n\n\n\nThe Inverse Error Function\n\n\n\npytorch\n\nmetal api\n\nmacos\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\nPeter Pham\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/autofs/index.html",
    "href": "posts/autofs/index.html",
    "title": "Mounting NAS using autofs",
    "section": "",
    "text": "autofs is a great tool to mount your network storage more efficiently on your Linux server. These are the benefits of using autofs:\n\nPersistence: Mounts persist across restarts\nOn-demand mounting: Mounts are only established when accessed, reducing boot time and system overhead\nReduced network traffic: Unneeded shares remain unmounted, saving bandwidth\nAutomatic retry: Automatically retries connection if the NAS is temporarily unavailable\nSupport for multiple protocols: Works with NFS (NFSv3, NFSv4), CIFS/SMB, FTP, and more\nGhost directories: The --ghost option creates placeholder directories for easier browsing\n\n\n\n\nInstall the required autofs. The following instruction assumes you have a Debian/Ubuntu OS.\n\nsudo apt update && sudo apt install -y autofs cifs-utils\n\nCreate your NAS credential file:\n\nsudo mkdir -p /etc/samba/credentials\nsudo nano /etc/samba/credentials/nas.cred\nnas.cred:\nusername=myuser\npassword=supersecretpassword\ndomain=MYDOMAIN   # optional, omit if not needed\nProtect your credentials from non-root users:\nsudo chmod 600 /etc/samba/credentials/nas.cred\nsudo chown root:root /etc/samba/credentials/nas.cred\n\nConfigure the master autofs master mapping:\n\nsudo nano /etc/auto.master\n/etc/auto.master:\n# Assuming you want to mount to `/mnt/smb` locally to your NAS:\n/mnt/smb  /etc/auto.cifs  --timeout=300 --ghost\n\nCreate a new map file:\n\nsudo nano /etc/auto.cifs\n/etc/auto.cifs:\n# Assuming your NAS is located at 192.168.2.223 with shares: vm, iso\nvm  -fstype=cifs,_netdev,credentials=/etc/samba/credentials/nas.cred,vers=3.1.1,serverino ://192.168.2.223/vm\niso -fstype=cifs,_netdev,credentials=/etc/samba/credentials/nas.cred,vers=3.1.1,serverino ://192.168.2.223/iso\n\nRestart autofs to pick up the updated configuration:\n\nsudo systemctl restart autofs\n\nCheck your files:\n\n# should show files from your NAS `vm` path!\ncd /mnt/smb/vm\n\n\n\n\n\nsudo systemctl status autofs\n\n\n\nsudo automount -f -v -v\n\n\n\nsudo mount -a\n\n\n\njournalctl -u autofs -f\n\n\n\n\n\n\n\n\n\n\nIssue\nSolution\n\n\n\n\nMount fails with “No such file or directory”\nCheck NAS IP and share name in map file\n\n\nMount fails with “Permission denied”\nVerify credentials file permissions and contents\n\n\nMount hangs\nAdd soft,intr options or check network connectivity\n\n\nFiles show as wrong owner\nAdd uid and gid options to map"
  },
  {
    "objectID": "posts/autofs/index.html#configuration",
    "href": "posts/autofs/index.html#configuration",
    "title": "Mounting NAS using autofs",
    "section": "",
    "text": "Install the required autofs. The following instruction assumes you have a Debian/Ubuntu OS.\n\nsudo apt update && sudo apt install -y autofs cifs-utils\n\nCreate your NAS credential file:\n\nsudo mkdir -p /etc/samba/credentials\nsudo nano /etc/samba/credentials/nas.cred\nnas.cred:\nusername=myuser\npassword=supersecretpassword\ndomain=MYDOMAIN   # optional, omit if not needed\nProtect your credentials from non-root users:\nsudo chmod 600 /etc/samba/credentials/nas.cred\nsudo chown root:root /etc/samba/credentials/nas.cred\n\nConfigure the master autofs master mapping:\n\nsudo nano /etc/auto.master\n/etc/auto.master:\n# Assuming you want to mount to `/mnt/smb` locally to your NAS:\n/mnt/smb  /etc/auto.cifs  --timeout=300 --ghost\n\nCreate a new map file:\n\nsudo nano /etc/auto.cifs\n/etc/auto.cifs:\n# Assuming your NAS is located at 192.168.2.223 with shares: vm, iso\nvm  -fstype=cifs,_netdev,credentials=/etc/samba/credentials/nas.cred,vers=3.1.1,serverino ://192.168.2.223/vm\niso -fstype=cifs,_netdev,credentials=/etc/samba/credentials/nas.cred,vers=3.1.1,serverino ://192.168.2.223/iso\n\nRestart autofs to pick up the updated configuration:\n\nsudo systemctl restart autofs\n\nCheck your files:\n\n# should show files from your NAS `vm` path!\ncd /mnt/smb/vm"
  },
  {
    "objectID": "posts/autofs/index.html#troubleshooting",
    "href": "posts/autofs/index.html#troubleshooting",
    "title": "Mounting NAS using autofs",
    "section": "",
    "text": "sudo systemctl status autofs\n\n\n\nsudo automount -f -v -v\n\n\n\nsudo mount -a\n\n\n\njournalctl -u autofs -f\n\n\n\n\n\n\n\n\n\n\nIssue\nSolution\n\n\n\n\nMount fails with “No such file or directory”\nCheck NAS IP and share name in map file\n\n\nMount fails with “Permission denied”\nVerify credentials file permissions and contents\n\n\nMount hangs\nAdd soft,intr options or check network connectivity\n\n\nFiles show as wrong owner\nAdd uid and gid options to map"
  },
  {
    "objectID": "posts/pytorch_erfinv/index.html",
    "href": "posts/pytorch_erfinv/index.html",
    "title": "The Inverse Error Function",
    "section": "",
    "text": "I happen to stumble upon a feature request to implement the metal backend for the \\(\\operatorname{erf}^{-1}(x)\\) in Pytorch. I thought it would be a good exercise to implement it myself to get a better understanding of defining custom torch ops for the MPS backend. (There’s also list of unimplemented torch ops for MPS)\n\n\nTo work on the inverse error function, we first need an understanding of the error function. The error function is defined as:\n\\[\n\\operatorname{erf}(x)=\\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^{2}} d t\n\\tag{1}\\]\nThe error function maps a real number in the domain \\((-\\infty, \\infty)\\) to a real number from \\((-1, 1)\\).\n\n\nCode\nimport math\n\nimport numpy as np\nimport plotly.express as px\nimport torch\n\n\nx = np.linspace(-5, 5, 1000)\ny = torch.erf(torch.tensor(x)).numpy()\n# add y as erf(x) label\nfig = px.line(x=x, y=y, labels={\"x\": \"x\", \"y\": \"erf(x)\"})\n\nfig.show()\n\n\n                            \n                                            \n\n\nThe inverse error function is defined as: \\[\n\\operatorname{erf}^{-1}(\\operatorname{erf}(x))=x\n\\tag{2}\\]\nThis means the \\(\\operatorname{erf}^{-1}(x)\\) will have a domain of \\((-1, 1)\\) and a range of \\((-\\infty, \\infty)\\).\nThere is no closed-form solution for the \\(\\operatorname{erf}^{-1}(x)\\) however it can be approximated using elementary function as proposed by Abramowitz and Stegun[1]. The approximation is given by:\n\\[\n\\operatorname{erf}^{-1}(x) \\approx \\operatorname{sgn}(x) \\sqrt{\\sqrt{\\left(\\frac{2}{\\pi a}+\\frac{\\ln(1-x^{2})}{2}\\right)^{2}-  \\frac{\\ln (1-x^{2})}{a}   }- (\\frac{2}{\\pi a}+\\frac{\\ln (1-x^{2})}{2})}\n\\tag{3}\\]\nwhere \\(a=0.147\\) or \\(a=0.140012\\) where the latter is more accurate around \\(x=0\\) for the error function but the former has a smaller maximum error for the error function. There was no analysis given for the maximum error rate of the \\(\\operatorname{erf}^{-1}(x)\\) so I will have to experiment with it myself.\nHere is a plot of the \\(\\operatorname{erf}^{-1}(x)\\) using the approximation method.\n\n\nCode\ndef erfinv(x, a=0.147):\n    \"\"\"\n    Compute the inverse error function using an approximation method\n    \"\"\"\n\n    # the Abravov fast approximation method\n    # compute the first term\n    term = np.sqrt(\n        np.sqrt(\n            (2 / (np.pi * a) + np.log(1 - x**2) / 2) ** 2 - np.log(1 - x**2) / a\n        )\n        - (2 / (np.pi * a) + np.log(1 - x**2) / 2)\n    )\n    # compute the sign\n    sign = 1 if x &gt; 0 else -1\n    y =  sign * term\n\n    return y\n\n\nx = np.linspace(-1, 1, 1000)\n# Vectorize the erfinv function\nerfinv_vec = np.vectorize(erfinv)\ny_rapid = erfinv_vec(x)\n\n\n\n\nCode\n# Add y as erfinv(x) label\nfig = px.line(x=x, y=y_rapid, labels={\"x\": \"x\", \"y\": \"erfinv(x)\"})\nfig.show()\n\n\n                            \n                                            \n\n\nLet’s compute the MSE (ignoring inf values) between the approximation versus the torch.erfinv implementation using:\n\\(a = 0.147\\)\n\n\nCode\nx = np.linspace(-1, 1, 1000)\ny_rapid = erfinv_vec(x)\ny_torch = torch.erfinv(torch.tensor(x)).numpy()\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid)\nx_mask = x[mask]\ny_rapid = y_rapid[mask]\ny_torch = y_torch[mask]\nmse = np.mean((y_rapid - y_torch) ** 2)\nprint(f\"MSE: {mse}\")\n\n\n\n\nCode\nprint(f\"MSE: {mse}\")\nmax_error_idx = np.argmax(np.abs(y_rapid - y_torch))\n\nprint(\n    f\"Max Error: {np.max(np.abs(y_rapid - y_torch))} at x: {x_mask[max_error_idx]}, index: {max_error_idx}\"\n)\nprint(f\"y_rapid: {y_rapid[max_error_idx]}, y_torch: {y_torch[max_error_idx]}\")\n\n\nMSE: 1.7424258166728075e-07\nMax Error: 0.003953770269555346 at x: -0.997997997997998, index: 0\ny_rapid: -2.180960329841855, y_torch: -2.1849141001114103\n\n\nNow repeat the experiment but using:\n\\(a=0.140012\\)\n\n\nCode\ny_rapid2 = erfinv_vec(x, a=0.140012)\ny_torch = torch.erfinv(torch.tensor(x)).numpy()\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid2)\nx_mask = x[mask]\ny_rapid2 = y_rapid2[mask]\ny_torch = y_torch[mask]\n\n\n\n\nCode\nmse = np.mean((y_rapid2 - y_torch) ** 2)\nprint(f\"MSE: {mse}\")\nmax_error_idx = np.argmax(np.abs(y_rapid2 - y_torch))\nprint(f\"Max Error: {np.max(np.abs(y_rapid2 - y_torch))} at x: {x_mask[max_error_idx]}\")\nprint(f\"y_rapid2: {y_rapid2[max_error_idx]}, y_torch: {y_torch[max_error_idx]}\")\n# max error index and x value\n\n\nMSE: 8.462108415214081e-07\nMax Error: 0.007140781441233646 at x: -0.997997997997998\ny_rapid2: -2.1777733186701766, y_torch: -2.1849141001114103\n\n\nBoth methods have the worst performance at around \\(x=-1\\) which is expected since the \\(\\operatorname{erf}^{-1}(x)\\) is asymptotic at \\(x=-1\\). I will use \\(a=0.147\\) for the approximation method since it has a smaller maximum error and also smaller MSE compared to the torch.erfinv implementation.\n\n\n\nNow that we have a decent approximation of the inverse error function, we can implement it in pytorch.\n\n\nMy experience with compiling pytorch from source was more challenging on macOS compared to Ubuntu 22.04.\nI used this guide and in addition I encountered 2 errors that weren’t covered in the guide.\n\nI had a mismatched installation of protoc (protobuf) both from conda and brew. I had to uninstall both of them.\nI encountered errors near the end related to cast-function-type-strict such as:\n\n\n/src/pytorch/torch/csrc/Generator.cpp /src/pytorch/torch/csrc/Generator.cpp:208:16: error: cast from ‘PyObject ()(THPGenerator , void )’ (aka ’_object ()(THPGenerator , void )‘) to ’getter’ (aka ’_object ()(_object , void )’) converts to incompatible function type [-Werror,-Wcast-function-type-strict] {“device”, (getter)THPGenerator_get_device, nullptr, nullptr, nullptr},\n\nI had to modify CMakeLists.txtto to comment out : -Werror=cast-function-type” CMAKE_CXX_FLAGS\nappend_cxx_flag_if_supported(\"-Wno-unused-but-set-variable\" CMAKE_CXX_FLAGS)\nappend_cxx_flag_if_supported(\"-Wno-maybe-uninitialized\" CMAKE_CXX_FLAGS)\nstring(APPEND CMAKE_CXX_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\nstring(APPEND CMAKE_LINKER_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\nappend_cxx_flag_if_supported(\"-fno-math-errno\" CMAKE_CXX_FLAGS)\nappend_cxx_flag_if_supported(\"-fno-trapping-math\" CMAKE_CXX_FLAGS)\nappend_cxx_flag_if_supported(\"-Werror=format\" CMAKE_CXX_FLAGS)\n\n# append_cxx_flag_if_supported(\"-Werror=cast-function-type\" CMAKE_CXX_FLAGS)\nThen finally I was able to compile pytorch from source. using the follow command:\nMACOSX_DEPLOYMENT_TARGET=13.0 CC=clang CXX=clang++ USE_MPS=1 USE_PYTORCH_METAL=1 \\\\\nDEBUG=1 python setup.py develop\n\n\n\n\n\nhttps://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraph\n\n\n\nFirst I have to try to reduce Equation 3 to avoid repeated calculation of terms in order to reduce the number of nodes in the compute graph.\nWe can create the following terms so that they are re-used in the graph:\n\\[\\begin{align*}\nA &= x^2 \\\\\nB &= \\log(1 - A) \\\\\nC &= \\frac{2}{\\pi a} + \\frac{B}{2} \\\\\n\\end{align*}\\]\nThen the Equation 3 can be re-written as: \\[\n\\operatorname{erfinv}(x) = \\operatorname{sgn}(x) \\sqrt{\\sqrt{C^2 - \\frac{B}{a}} - C}\n\\]\n\n\\(A\\) term requires 1 multiply node\n\\(B\\) term requires 1 log node and 1 subtract node\n\\(C\\) term requires 1 add node, 2 divisision nodes, 1 multiply node\n\\(\\operatorname{erfinv}(x)\\) requires 2 square root nodes, 2 subtract nodes, 1 multiply node, 1 division node\n\nthe \\(\\operatorname{sgn}(x)\\) term requires 4 nodes: greaterThan, selectPredicate, 2 multiply nodes\n\n\nThis translate to a total of 17 nodes (not counting constants) in the compute graph for the erfinv function.\n\n\n\n// constant tensors\nauto negOneTensor = [mpsGraph constantWithScalar:-1.0 dataType:inputTensor.dataType];\nauto zeroTensor = [mpsGraph constantWithScalar:0.0 dataType:inputTensor.dataType];\nauto halfTensor = [mpsGraph constantWithScalar:0.5 dataType:inputTensor.dataType];\nauto oneTensor = [mpsGraph constantWithScalar:1.0 dataType:inputTensor.dataType];\nauto twoTensor = [mpsGraph constantWithScalar:2.0 dataType:inputTensor.dataType];\nauto piTensor = [mpsGraph constantWithScalar:3.14159265358979323846264338327950288 dataType:inputTensor.dataType];\nauto aTensor = [mpsGraph constantWithScalar:0.147 dataType:inputTensor.dataType];\n\n\nauto A = [mpsGraph multiplicationWithPrimaryTensor:inputTensor secondaryTensor:inputTensor name:nil];\nauto B = [mpsGraph logarithmWithTensor:[mpsGraph subtractionWithPrimaryTensor:oneTensor\n                                                                    secondaryTensor:A\n                                                                                name:nil]\n                                        name:nil];\nauto C = [mpsGraph\n    additionWithPrimaryTensor:[mpsGraph divisionWithPrimaryTensor:twoTensor\n                                                    secondaryTensor:[mpsGraph multiplicationWithPrimaryTensor:piTensor\n                                                                                            secondaryTensor:aTensor\n                                                                                                        name:nil]\n                                                                name:nil]\n                secondaryTensor:[mpsGraph multiplicationWithPrimaryTensor:B secondaryTensor:halfTensor name:nil]\n                            name:nil];\nauto CSquared = [mpsGraph multiplicationWithPrimaryTensor:C secondaryTensor:C name:nil];\nauto CSquaredMinusBDivA = [mpsGraph subtractionWithPrimaryTensor:CSquared\n                                        secondaryTensor:[mpsGraph divisionWithPrimaryTensor:B\n                                                                            secondaryTensor:aTensor\n                                                                                        name:nil]\n                                                    name:nil];\nauto squareRootDiffTerm = [mpsGraph squareRootWithTensor:CSquaredMinusBDivA name:nil];\nauto finalDiff = [mpsGraph subtractionWithPrimaryTensor:squareRootDiffTerm secondaryTensor:C name:nil];\nauto finalSquareRoot = [mpsGraph squareRootWithTensor:finalDiff name:nil];\nauto predicateTensor = [mpsGraph greaterThanOrEqualToWithPrimaryTensor:inputTensor\n                                                        secondaryTensor:zeroTensor\n                                                                    name:nil];\nauto resultPositive = [mpsGraph multiplicationWithPrimaryTensor:finalSquareRoot secondaryTensor:oneTensor name:nil];\nauto resultNegative = [mpsGraph multiplicationWithPrimaryTensor:finalSquareRoot\n                                                secondaryTensor:negOneTensor\n                                                            name:nil];\nreturn [mpsGraph selectWithPredicateTensor:predicateTensor\n                        truePredicateTensor:resultPositive\n                        falsePredicateTensor:resultNegative\n                                        name:nil];\n\nHere’s a quick benchmark of the MPS compute (M1 macbookpro 16” 16 GB) vs CPU for the erfinv function:\n\n\nCode\nimport torch\nx = torch.arange(-1, 1, 0.00001)\nx = x.to(\"mps\")\n# measure MPS compute time\ntime = %timeit -o -q  torch.erfinv(x)\nmps_time = time.average\nprint(\"MPS torch.erfinv time: \", mps_time)\nx = x.to(\"cpu\")\n# measure CPU compute time by calling torch.erfinv but storing it to y_cpu\ntime = %timeit -o -q torch.erfinv(x)\ncpu_time = time.average\nprint(\"CPU torch.erfinv time: \", cpu_time)\nprint(f\"MPS torch.erfinv is {cpu_time/mps_time*100} percent faster than CPU torch.erfinv\")\n\n# compute MSE between y_cpu and y_mps\nx = x.to(\"mps\")\ny_mps = torch.erfinv(x)\ny_cpu = torch.erfinv(x.to(\"cpu\"))\nmask = torch.isfinite(y_cpu) & torch.isfinite(y_mps.to(\"cpu\"))\nmse = torch.square(y_cpu[mask] - y_mps[mask].to(\"cpu\")).mean()\nprint(\"MSE between MPS and CPU torch.erfinv: \", mse)\n\n\nMPS torch.erfinv time:  9.036031367140822e-07\nCPU torch.erfinv time:  0.0004053320000010509\nMPS torch.erfinv is 44857.303337284226 percent faster than CPU torch.erfinv\nMSE between MPS and CPU torch.erfinv:  tensor(4.0603e-14)\n\n\nI thought I was done until I added erfinv into those 2 test cases.\npython3 test/test_mps.py TestNLLLoss.test_unary_ops\npython3 test/test_unary_ufuncs.py TestUnaryUfuncs.unary_mem_overlap_cases\nI found that the metal compute graph accuracy was not good enough to pass the test for TestNLLLoss.test_unary_ops where it could only pass 70% of the test. I ended up adding 2 processing step to include: - 2 iterations of the Newton-Raphson method to improve the accuracy of the MPS compute graph. - Logic to set to +/- inf if input is +/- 1.0 since the approximation be off at the boundary.\nBelow was the addition to the MPSGraph unary compute graph for erfinv:\n   // add 2 steps of Newton-Raphson iteration to improve accuracy\n    // adopted from\n    // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L191\n\n    auto currentEstimated = estimated;\n    for (int i = 0; i &lt; 2; ++i) {\n      auto negEstimated = [mpsGraph multiplicationWithPrimaryTensor:currentEstimated\n                                                    secondaryTensor:negOneTensor\n                                                               name:nil];\n      auto estimatedSquared = [mpsGraph multiplicationWithPrimaryTensor:negEstimated\n                                                        secondaryTensor:currentEstimated\n                                                                   name:nil];\n      auto estimatedSquaredExp = [mpsGraph exponentWithTensor:estimatedSquared name:nil];\n      auto twoDivSquareRootPi = [mpsGraph divisionWithPrimaryTensor:twoTensor\n                                                    secondaryTensor:piSquareRootTensor\n                                                               name:nil];\n      auto gradientDenominator = [mpsGraph multiplicationWithPrimaryTensor:twoDivSquareRootPi\n                                                           secondaryTensor:estimatedSquaredExp\n                                                                      name:nil];\n      auto changeErf = [mpsGraph subtractionWithPrimaryTensor:[mpsGraph erfWithTensor:currentEstimated name:nil]\n                                              secondaryTensor:inputTensor\n                                                         name:nil];\n      auto gradient = [mpsGraph divisionWithPrimaryTensor:changeErf secondaryTensor:gradientDenominator name:nil];\n      currentEstimated = [mpsGraph subtractionWithPrimaryTensor:currentEstimated secondaryTensor:gradient name:nil];\n    }\n\n    // post processing step to check if we have exactly +1/-1 then we should map to infinity/-infinity\n    // this is because the this algorithm might push us on the wrong side of the asymptote due to rounding\n    auto onePredicate = [mpsGraph equalWithPrimaryTensor:inputTensor secondaryTensor:oneTensor name:nil];\n    auto negOnePredicate = [mpsGraph equalWithPrimaryTensor:inputTensor secondaryTensor:negOneTensor name:nil];\n\n    auto resultWithInfinity = [mpsGraph selectWithPredicateTensor:onePredicate\n                                              truePredicateTensor:infinityTensor\n                                             falsePredicateTensor:currentEstimated\n                                                             name:nil];\n    return [mpsGraph selectWithPredicateTensor:negOnePredicate\n                           truePredicateTensor:negInfinityTensor\n                          falsePredicateTensor:resultWithInfinity\n                                          name:nil];\nAdding the above code allows me to pass the pytorch automatic testing 100%.\n➜ python3 test/test_mps.py TestNLLLoss.test_unary_ops\n.\n----------------------------------------------------------------------\nRan 1 test in 0.283s\n\nOK\nUnfortuantely, I discovered that the current algorithm uses too much memory. I have to optimize the MPS compute graph further before this function could be of practical use. I will go back and update this blog and reopen my PR once I have a better solution.\n** UPDATE July 19 20123 ** I had created another PR where I used raw metal kernel for speed up instead of the MPS graph api. This PR is currently under review."
  },
  {
    "objectID": "posts/pytorch_erfinv/index.html#math",
    "href": "posts/pytorch_erfinv/index.html#math",
    "title": "The Inverse Error Function",
    "section": "",
    "text": "To work on the inverse error function, we first need an understanding of the error function. The error function is defined as:\n\\[\n\\operatorname{erf}(x)=\\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^{2}} d t\n\\tag{1}\\]\nThe error function maps a real number in the domain \\((-\\infty, \\infty)\\) to a real number from \\((-1, 1)\\).\n\n\nCode\nimport math\n\nimport numpy as np\nimport plotly.express as px\nimport torch\n\n\nx = np.linspace(-5, 5, 1000)\ny = torch.erf(torch.tensor(x)).numpy()\n# add y as erf(x) label\nfig = px.line(x=x, y=y, labels={\"x\": \"x\", \"y\": \"erf(x)\"})\n\nfig.show()\n\n\n                            \n                                            \n\n\nThe inverse error function is defined as: \\[\n\\operatorname{erf}^{-1}(\\operatorname{erf}(x))=x\n\\tag{2}\\]\nThis means the \\(\\operatorname{erf}^{-1}(x)\\) will have a domain of \\((-1, 1)\\) and a range of \\((-\\infty, \\infty)\\).\nThere is no closed-form solution for the \\(\\operatorname{erf}^{-1}(x)\\) however it can be approximated using elementary function as proposed by Abramowitz and Stegun[1]. The approximation is given by:\n\\[\n\\operatorname{erf}^{-1}(x) \\approx \\operatorname{sgn}(x) \\sqrt{\\sqrt{\\left(\\frac{2}{\\pi a}+\\frac{\\ln(1-x^{2})}{2}\\right)^{2}-  \\frac{\\ln (1-x^{2})}{a}   }- (\\frac{2}{\\pi a}+\\frac{\\ln (1-x^{2})}{2})}\n\\tag{3}\\]\nwhere \\(a=0.147\\) or \\(a=0.140012\\) where the latter is more accurate around \\(x=0\\) for the error function but the former has a smaller maximum error for the error function. There was no analysis given for the maximum error rate of the \\(\\operatorname{erf}^{-1}(x)\\) so I will have to experiment with it myself.\nHere is a plot of the \\(\\operatorname{erf}^{-1}(x)\\) using the approximation method.\n\n\nCode\ndef erfinv(x, a=0.147):\n    \"\"\"\n    Compute the inverse error function using an approximation method\n    \"\"\"\n\n    # the Abravov fast approximation method\n    # compute the first term\n    term = np.sqrt(\n        np.sqrt(\n            (2 / (np.pi * a) + np.log(1 - x**2) / 2) ** 2 - np.log(1 - x**2) / a\n        )\n        - (2 / (np.pi * a) + np.log(1 - x**2) / 2)\n    )\n    # compute the sign\n    sign = 1 if x &gt; 0 else -1\n    y =  sign * term\n\n    return y\n\n\nx = np.linspace(-1, 1, 1000)\n# Vectorize the erfinv function\nerfinv_vec = np.vectorize(erfinv)\ny_rapid = erfinv_vec(x)\n\n\n\n\nCode\n# Add y as erfinv(x) label\nfig = px.line(x=x, y=y_rapid, labels={\"x\": \"x\", \"y\": \"erfinv(x)\"})\nfig.show()\n\n\n                            \n                                            \n\n\nLet’s compute the MSE (ignoring inf values) between the approximation versus the torch.erfinv implementation using:\n\\(a = 0.147\\)\n\n\nCode\nx = np.linspace(-1, 1, 1000)\ny_rapid = erfinv_vec(x)\ny_torch = torch.erfinv(torch.tensor(x)).numpy()\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid)\nx_mask = x[mask]\ny_rapid = y_rapid[mask]\ny_torch = y_torch[mask]\nmse = np.mean((y_rapid - y_torch) ** 2)\nprint(f\"MSE: {mse}\")\n\n\n\n\nCode\nprint(f\"MSE: {mse}\")\nmax_error_idx = np.argmax(np.abs(y_rapid - y_torch))\n\nprint(\n    f\"Max Error: {np.max(np.abs(y_rapid - y_torch))} at x: {x_mask[max_error_idx]}, index: {max_error_idx}\"\n)\nprint(f\"y_rapid: {y_rapid[max_error_idx]}, y_torch: {y_torch[max_error_idx]}\")\n\n\nMSE: 1.7424258166728075e-07\nMax Error: 0.003953770269555346 at x: -0.997997997997998, index: 0\ny_rapid: -2.180960329841855, y_torch: -2.1849141001114103\n\n\nNow repeat the experiment but using:\n\\(a=0.140012\\)\n\n\nCode\ny_rapid2 = erfinv_vec(x, a=0.140012)\ny_torch = torch.erfinv(torch.tensor(x)).numpy()\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid2)\nx_mask = x[mask]\ny_rapid2 = y_rapid2[mask]\ny_torch = y_torch[mask]\n\n\n\n\nCode\nmse = np.mean((y_rapid2 - y_torch) ** 2)\nprint(f\"MSE: {mse}\")\nmax_error_idx = np.argmax(np.abs(y_rapid2 - y_torch))\nprint(f\"Max Error: {np.max(np.abs(y_rapid2 - y_torch))} at x: {x_mask[max_error_idx]}\")\nprint(f\"y_rapid2: {y_rapid2[max_error_idx]}, y_torch: {y_torch[max_error_idx]}\")\n# max error index and x value\n\n\nMSE: 8.462108415214081e-07\nMax Error: 0.007140781441233646 at x: -0.997997997997998\ny_rapid2: -2.1777733186701766, y_torch: -2.1849141001114103\n\n\nBoth methods have the worst performance at around \\(x=-1\\) which is expected since the \\(\\operatorname{erf}^{-1}(x)\\) is asymptotic at \\(x=-1\\). I will use \\(a=0.147\\) for the approximation method since it has a smaller maximum error and also smaller MSE compared to the torch.erfinv implementation."
  },
  {
    "objectID": "posts/pytorch_erfinv/index.html#pytorch-implementation",
    "href": "posts/pytorch_erfinv/index.html#pytorch-implementation",
    "title": "The Inverse Error Function",
    "section": "",
    "text": "Now that we have a decent approximation of the inverse error function, we can implement it in pytorch.\n\n\nMy experience with compiling pytorch from source was more challenging on macOS compared to Ubuntu 22.04.\nI used this guide and in addition I encountered 2 errors that weren’t covered in the guide.\n\nI had a mismatched installation of protoc (protobuf) both from conda and brew. I had to uninstall both of them.\nI encountered errors near the end related to cast-function-type-strict such as:\n\n\n/src/pytorch/torch/csrc/Generator.cpp /src/pytorch/torch/csrc/Generator.cpp:208:16: error: cast from ‘PyObject ()(THPGenerator , void )’ (aka ’_object ()(THPGenerator , void )‘) to ’getter’ (aka ’_object ()(_object , void )’) converts to incompatible function type [-Werror,-Wcast-function-type-strict] {“device”, (getter)THPGenerator_get_device, nullptr, nullptr, nullptr},\n\nI had to modify CMakeLists.txtto to comment out : -Werror=cast-function-type” CMAKE_CXX_FLAGS\nappend_cxx_flag_if_supported(\"-Wno-unused-but-set-variable\" CMAKE_CXX_FLAGS)\nappend_cxx_flag_if_supported(\"-Wno-maybe-uninitialized\" CMAKE_CXX_FLAGS)\nstring(APPEND CMAKE_CXX_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\nstring(APPEND CMAKE_LINKER_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\nappend_cxx_flag_if_supported(\"-fno-math-errno\" CMAKE_CXX_FLAGS)\nappend_cxx_flag_if_supported(\"-fno-trapping-math\" CMAKE_CXX_FLAGS)\nappend_cxx_flag_if_supported(\"-Werror=format\" CMAKE_CXX_FLAGS)\n\n# append_cxx_flag_if_supported(\"-Werror=cast-function-type\" CMAKE_CXX_FLAGS)\nThen finally I was able to compile pytorch from source. using the follow command:\nMACOSX_DEPLOYMENT_TARGET=13.0 CC=clang CXX=clang++ USE_MPS=1 USE_PYTORCH_METAL=1 \\\\\nDEBUG=1 python setup.py develop"
  },
  {
    "objectID": "posts/pytorch_erfinv/index.html#metal-api",
    "href": "posts/pytorch_erfinv/index.html#metal-api",
    "title": "The Inverse Error Function",
    "section": "",
    "text": "https://developer.apple.com/documentation/metalperformanceshadersgraph/mpsgraph\n\n\n\nFirst I have to try to reduce Equation 3 to avoid repeated calculation of terms in order to reduce the number of nodes in the compute graph.\nWe can create the following terms so that they are re-used in the graph:\n\\[\\begin{align*}\nA &= x^2 \\\\\nB &= \\log(1 - A) \\\\\nC &= \\frac{2}{\\pi a} + \\frac{B}{2} \\\\\n\\end{align*}\\]\nThen the Equation 3 can be re-written as: \\[\n\\operatorname{erfinv}(x) = \\operatorname{sgn}(x) \\sqrt{\\sqrt{C^2 - \\frac{B}{a}} - C}\n\\]\n\n\\(A\\) term requires 1 multiply node\n\\(B\\) term requires 1 log node and 1 subtract node\n\\(C\\) term requires 1 add node, 2 divisision nodes, 1 multiply node\n\\(\\operatorname{erfinv}(x)\\) requires 2 square root nodes, 2 subtract nodes, 1 multiply node, 1 division node\n\nthe \\(\\operatorname{sgn}(x)\\) term requires 4 nodes: greaterThan, selectPredicate, 2 multiply nodes\n\n\nThis translate to a total of 17 nodes (not counting constants) in the compute graph for the erfinv function.\n\n\n\n// constant tensors\nauto negOneTensor = [mpsGraph constantWithScalar:-1.0 dataType:inputTensor.dataType];\nauto zeroTensor = [mpsGraph constantWithScalar:0.0 dataType:inputTensor.dataType];\nauto halfTensor = [mpsGraph constantWithScalar:0.5 dataType:inputTensor.dataType];\nauto oneTensor = [mpsGraph constantWithScalar:1.0 dataType:inputTensor.dataType];\nauto twoTensor = [mpsGraph constantWithScalar:2.0 dataType:inputTensor.dataType];\nauto piTensor = [mpsGraph constantWithScalar:3.14159265358979323846264338327950288 dataType:inputTensor.dataType];\nauto aTensor = [mpsGraph constantWithScalar:0.147 dataType:inputTensor.dataType];\n\n\nauto A = [mpsGraph multiplicationWithPrimaryTensor:inputTensor secondaryTensor:inputTensor name:nil];\nauto B = [mpsGraph logarithmWithTensor:[mpsGraph subtractionWithPrimaryTensor:oneTensor\n                                                                    secondaryTensor:A\n                                                                                name:nil]\n                                        name:nil];\nauto C = [mpsGraph\n    additionWithPrimaryTensor:[mpsGraph divisionWithPrimaryTensor:twoTensor\n                                                    secondaryTensor:[mpsGraph multiplicationWithPrimaryTensor:piTensor\n                                                                                            secondaryTensor:aTensor\n                                                                                                        name:nil]\n                                                                name:nil]\n                secondaryTensor:[mpsGraph multiplicationWithPrimaryTensor:B secondaryTensor:halfTensor name:nil]\n                            name:nil];\nauto CSquared = [mpsGraph multiplicationWithPrimaryTensor:C secondaryTensor:C name:nil];\nauto CSquaredMinusBDivA = [mpsGraph subtractionWithPrimaryTensor:CSquared\n                                        secondaryTensor:[mpsGraph divisionWithPrimaryTensor:B\n                                                                            secondaryTensor:aTensor\n                                                                                        name:nil]\n                                                    name:nil];\nauto squareRootDiffTerm = [mpsGraph squareRootWithTensor:CSquaredMinusBDivA name:nil];\nauto finalDiff = [mpsGraph subtractionWithPrimaryTensor:squareRootDiffTerm secondaryTensor:C name:nil];\nauto finalSquareRoot = [mpsGraph squareRootWithTensor:finalDiff name:nil];\nauto predicateTensor = [mpsGraph greaterThanOrEqualToWithPrimaryTensor:inputTensor\n                                                        secondaryTensor:zeroTensor\n                                                                    name:nil];\nauto resultPositive = [mpsGraph multiplicationWithPrimaryTensor:finalSquareRoot secondaryTensor:oneTensor name:nil];\nauto resultNegative = [mpsGraph multiplicationWithPrimaryTensor:finalSquareRoot\n                                                secondaryTensor:negOneTensor\n                                                            name:nil];\nreturn [mpsGraph selectWithPredicateTensor:predicateTensor\n                        truePredicateTensor:resultPositive\n                        falsePredicateTensor:resultNegative\n                                        name:nil];\n\nHere’s a quick benchmark of the MPS compute (M1 macbookpro 16” 16 GB) vs CPU for the erfinv function:\n\n\nCode\nimport torch\nx = torch.arange(-1, 1, 0.00001)\nx = x.to(\"mps\")\n# measure MPS compute time\ntime = %timeit -o -q  torch.erfinv(x)\nmps_time = time.average\nprint(\"MPS torch.erfinv time: \", mps_time)\nx = x.to(\"cpu\")\n# measure CPU compute time by calling torch.erfinv but storing it to y_cpu\ntime = %timeit -o -q torch.erfinv(x)\ncpu_time = time.average\nprint(\"CPU torch.erfinv time: \", cpu_time)\nprint(f\"MPS torch.erfinv is {cpu_time/mps_time*100} percent faster than CPU torch.erfinv\")\n\n# compute MSE between y_cpu and y_mps\nx = x.to(\"mps\")\ny_mps = torch.erfinv(x)\ny_cpu = torch.erfinv(x.to(\"cpu\"))\nmask = torch.isfinite(y_cpu) & torch.isfinite(y_mps.to(\"cpu\"))\nmse = torch.square(y_cpu[mask] - y_mps[mask].to(\"cpu\")).mean()\nprint(\"MSE between MPS and CPU torch.erfinv: \", mse)\n\n\nMPS torch.erfinv time:  9.036031367140822e-07\nCPU torch.erfinv time:  0.0004053320000010509\nMPS torch.erfinv is 44857.303337284226 percent faster than CPU torch.erfinv\nMSE between MPS and CPU torch.erfinv:  tensor(4.0603e-14)\n\n\nI thought I was done until I added erfinv into those 2 test cases.\npython3 test/test_mps.py TestNLLLoss.test_unary_ops\npython3 test/test_unary_ufuncs.py TestUnaryUfuncs.unary_mem_overlap_cases\nI found that the metal compute graph accuracy was not good enough to pass the test for TestNLLLoss.test_unary_ops where it could only pass 70% of the test. I ended up adding 2 processing step to include: - 2 iterations of the Newton-Raphson method to improve the accuracy of the MPS compute graph. - Logic to set to +/- inf if input is +/- 1.0 since the approximation be off at the boundary.\nBelow was the addition to the MPSGraph unary compute graph for erfinv:\n   // add 2 steps of Newton-Raphson iteration to improve accuracy\n    // adopted from\n    // https://github.com/pytorch/pytorch/blob/4154c8ea159fdaecc71ee9af820ac956193c875b/aten/src/ATen/native/Math.h#L191\n\n    auto currentEstimated = estimated;\n    for (int i = 0; i &lt; 2; ++i) {\n      auto negEstimated = [mpsGraph multiplicationWithPrimaryTensor:currentEstimated\n                                                    secondaryTensor:negOneTensor\n                                                               name:nil];\n      auto estimatedSquared = [mpsGraph multiplicationWithPrimaryTensor:negEstimated\n                                                        secondaryTensor:currentEstimated\n                                                                   name:nil];\n      auto estimatedSquaredExp = [mpsGraph exponentWithTensor:estimatedSquared name:nil];\n      auto twoDivSquareRootPi = [mpsGraph divisionWithPrimaryTensor:twoTensor\n                                                    secondaryTensor:piSquareRootTensor\n                                                               name:nil];\n      auto gradientDenominator = [mpsGraph multiplicationWithPrimaryTensor:twoDivSquareRootPi\n                                                           secondaryTensor:estimatedSquaredExp\n                                                                      name:nil];\n      auto changeErf = [mpsGraph subtractionWithPrimaryTensor:[mpsGraph erfWithTensor:currentEstimated name:nil]\n                                              secondaryTensor:inputTensor\n                                                         name:nil];\n      auto gradient = [mpsGraph divisionWithPrimaryTensor:changeErf secondaryTensor:gradientDenominator name:nil];\n      currentEstimated = [mpsGraph subtractionWithPrimaryTensor:currentEstimated secondaryTensor:gradient name:nil];\n    }\n\n    // post processing step to check if we have exactly +1/-1 then we should map to infinity/-infinity\n    // this is because the this algorithm might push us on the wrong side of the asymptote due to rounding\n    auto onePredicate = [mpsGraph equalWithPrimaryTensor:inputTensor secondaryTensor:oneTensor name:nil];\n    auto negOnePredicate = [mpsGraph equalWithPrimaryTensor:inputTensor secondaryTensor:negOneTensor name:nil];\n\n    auto resultWithInfinity = [mpsGraph selectWithPredicateTensor:onePredicate\n                                              truePredicateTensor:infinityTensor\n                                             falsePredicateTensor:currentEstimated\n                                                             name:nil];\n    return [mpsGraph selectWithPredicateTensor:negOnePredicate\n                           truePredicateTensor:negInfinityTensor\n                          falsePredicateTensor:resultWithInfinity\n                                          name:nil];\nAdding the above code allows me to pass the pytorch automatic testing 100%.\n➜ python3 test/test_mps.py TestNLLLoss.test_unary_ops\n.\n----------------------------------------------------------------------\nRan 1 test in 0.283s\n\nOK\nUnfortuantely, I discovered that the current algorithm uses too much memory. I have to optimize the MPS compute graph further before this function could be of practical use. I will go back and update this blog and reopen my PR once I have a better solution.\n** UPDATE July 19 20123 ** I had created another PR where I used raw metal kernel for speed up instead of the MPS graph api. This PR is currently under review."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Peter, a software engineer based in Portland, OR."
  }
]