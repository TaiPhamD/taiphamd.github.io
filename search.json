[
  {
    "objectID": "posts/pytorch_erfinv/index.html",
    "href": "posts/pytorch_erfinv/index.html",
    "title": "The Inverse Error Function",
    "section": "",
    "text": "I happen to stumble upon a feature request to implement the metal backend for the inverse error function. I thought it would be a good exercise to implement it myself.\n\n\nTo work on the inverse error function, we first need an understanding of the error function. The error function is defined as:\n\\[\n\\operatorname{erf}(x)=\\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^{2}} d t\n\\]\nThe error function maps a real number in the domain \\((-\\infty, \\infty)\\) to a real number from \\((-1, 1)\\).\n\n\nCode\nimport math\n\nimport numpy as np\nimport plotly.express as px\nfrom scipy.special import erf, erfinv as scipy_erfinv\n\nx = np.linspace(-5, 5, 1000)\ny = erf(x)\n# add y as erf(x) label\nfig = px.line(x=x, y=y, labels={\"x\": \"x\", \"y\": \"erf(x)\"})\n\nfig.show()\n\n\n\n                                                \n\n\nThe inverse error function is defined as: \\[\n\\operatorname{erf}^{-1}(\\operatorname{erf}(x))=x\n\\]\nThis means the inverse error function will have a domain of \\((-1, 1)\\) and a range of \\((-\\infty, \\infty)\\).\nThere is no closed-form solution for the inverse error function however it can be approximated using elementary function as proposed by Abramowitz and Stegun[1]. The approximation is given by:\n\\[\n\\operatorname{erf}^{-1}(x) \\approx \\operatorname{sgn}(x) \\sqrt{\\sqrt{\\left(\\frac{2}{\\pi a}+\\frac{\\ln(1-x^{2})}{2}\\right)^{2}-  \\frac{\\ln (1-x^{2})}{a}   }- (\\frac{2}{\\pi a}+\\frac{\\ln (1-x^{2})}{2})}\n\\]\nwhere \\(a=0.147\\) or \\(a=0.140012\\) where the latter is more accurate around \\(x=0\\) for the error function but the former has a smaller maximum error for the error function. There was no analysis given for the maximum error rate of the inverse error function so I will have to experiment with it myself.\nI am going to re-draw that same graph but using the approximation method.\n\n\nCode\ndef erfinv(x):\n    \"\"\"\n    Compute the inverse error function using an approximation method\n    \"\"\"\n\n    # the Abravov fast approximation method\n    a = 0.147\n    # compute the first term\n    term = np.sqrt(\n        np.sqrt(\n            (2 / (np.pi * a) + np.log(1 - x**2) / 2) ** 2 - np.log(1 - x**2) / a\n        )\n        - (2 / (np.pi * a) + np.log(1 - x**2) / 2)\n    )\n    # compute the sign\n    sign = 1 if x &gt; 0 else -1\n    return sign * term\n\n\nx = np.linspace(-1, 1, 1000)\n# Vectorize the erfinv function\nerfinv_vec = np.vectorize(erfinv)\ny_rapid = erfinv_vec(x)\n\n# Add y as erfinv(x) label\nfig = px.line(x=x, y=y_rapid, labels={\"x\": \"x\", \"y\": \"erfinv(x)\"})\n\nfig.show()\n\n\nLet’s compute the MSE between approximation and the scipy implementation using \\(a = 0.147\\)\n\n\nCode\ndef erfinv(x, a=0.147):\n    \"\"\"\n    Compute the inverse error function using an approximation method\n    \"\"\"\n\n    # the Abravov fast approximation method\n    # compute the first term\n    term = np.sqrt(\n        np.sqrt(\n            (2 / (np.pi * a) + np.log(1 - x**2) / 2) ** 2 - np.log(1 - x**2) / a\n        )\n        - (2 / (np.pi * a) + np.log(1 - x**2) / 2)\n    )\n    # compute the sign\n    sign = 1 if x &gt; 0 else -1\n    return sign * term\n\n\n# Vectorize the erf_rapid function\nerfinv_vec = np.vectorize(erfinv)\n\nx = np.linspace(-1, 1, 1000)\ny_rapid = erfinv_vec(x)\ny_scipy = scipy_erfinv(x)\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid)\ny_rapid = y_rapid[mask]\ny_scipy = y_scipy[mask]\n\nmse = np.mean((y_rapid - y_scipy) ** 2)\nprint(f\"MSE: {mse}\")\n\n\n\n\nCode\nprint(f\"MSE: {mse}\")\nprint(f\"Max Error: {np.max(np.abs(y_rapid - y_scipy))}\")\n\n\nMSE: 1.7424258166713035e-07\nMax Error: 0.003953770269546908\n\n\nNow re-peat the experiment but using \\(a=0.140012\\)\n\n\nCode\ny_rapid2 = erfinv_vec(x, a=0.140012)\ny_scipy = scipy_erfinv(x)\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid2)\ny_rapid2 = y_rapid2[mask]\ny_scipy = y_scipy[mask]\n\n\n\n\nCode\nmse = np.mean((y_rapid2 - y_scipy) ** 2)\nprint(f\"MSE: {mse}\")\nprint(f\"Max Error: {np.max(np.abs(y_rapid2 - y_scipy))}\")\n\n\nMSE: 8.462108415211391e-07\nMax Error: 0.007140781441225208\n\n\n\n\n\nNow that we have a decent approximation of the inverse error function, we can implement it in pytorch. I will be using the \\(a=0.147\\) approximation method since it gives a less maximum error.\n[TO DO: Add pytorch implementation]"
  },
  {
    "objectID": "posts/pytorch_erfinv/index.html#math",
    "href": "posts/pytorch_erfinv/index.html#math",
    "title": "The Inverse Error Function",
    "section": "",
    "text": "To work on the inverse error function, we first need an understanding of the error function. The error function is defined as:\n\\[\n\\operatorname{erf}(x)=\\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^{2}} d t\n\\]\nThe error function maps a real number in the domain \\((-\\infty, \\infty)\\) to a real number from \\((-1, 1)\\).\n\n\nCode\nimport math\n\nimport numpy as np\nimport plotly.express as px\nfrom scipy.special import erf, erfinv as scipy_erfinv\n\nx = np.linspace(-5, 5, 1000)\ny = erf(x)\n# add y as erf(x) label\nfig = px.line(x=x, y=y, labels={\"x\": \"x\", \"y\": \"erf(x)\"})\n\nfig.show()\n\n\n\n                                                \n\n\nThe inverse error function is defined as: \\[\n\\operatorname{erf}^{-1}(\\operatorname{erf}(x))=x\n\\]\nThis means the inverse error function will have a domain of \\((-1, 1)\\) and a range of \\((-\\infty, \\infty)\\).\nThere is no closed-form solution for the inverse error function however it can be approximated using elementary function as proposed by Abramowitz and Stegun[1]. The approximation is given by:\n\\[\n\\operatorname{erf}^{-1}(x) \\approx \\operatorname{sgn}(x) \\sqrt{\\sqrt{\\left(\\frac{2}{\\pi a}+\\frac{\\ln(1-x^{2})}{2}\\right)^{2}-  \\frac{\\ln (1-x^{2})}{a}   }- (\\frac{2}{\\pi a}+\\frac{\\ln (1-x^{2})}{2})}\n\\]\nwhere \\(a=0.147\\) or \\(a=0.140012\\) where the latter is more accurate around \\(x=0\\) for the error function but the former has a smaller maximum error for the error function. There was no analysis given for the maximum error rate of the inverse error function so I will have to experiment with it myself.\nI am going to re-draw that same graph but using the approximation method.\n\n\nCode\ndef erfinv(x):\n    \"\"\"\n    Compute the inverse error function using an approximation method\n    \"\"\"\n\n    # the Abravov fast approximation method\n    a = 0.147\n    # compute the first term\n    term = np.sqrt(\n        np.sqrt(\n            (2 / (np.pi * a) + np.log(1 - x**2) / 2) ** 2 - np.log(1 - x**2) / a\n        )\n        - (2 / (np.pi * a) + np.log(1 - x**2) / 2)\n    )\n    # compute the sign\n    sign = 1 if x &gt; 0 else -1\n    return sign * term\n\n\nx = np.linspace(-1, 1, 1000)\n# Vectorize the erfinv function\nerfinv_vec = np.vectorize(erfinv)\ny_rapid = erfinv_vec(x)\n\n# Add y as erfinv(x) label\nfig = px.line(x=x, y=y_rapid, labels={\"x\": \"x\", \"y\": \"erfinv(x)\"})\n\nfig.show()\n\n\nLet’s compute the MSE between approximation and the scipy implementation using \\(a = 0.147\\)\n\n\nCode\ndef erfinv(x, a=0.147):\n    \"\"\"\n    Compute the inverse error function using an approximation method\n    \"\"\"\n\n    # the Abravov fast approximation method\n    # compute the first term\n    term = np.sqrt(\n        np.sqrt(\n            (2 / (np.pi * a) + np.log(1 - x**2) / 2) ** 2 - np.log(1 - x**2) / a\n        )\n        - (2 / (np.pi * a) + np.log(1 - x**2) / 2)\n    )\n    # compute the sign\n    sign = 1 if x &gt; 0 else -1\n    return sign * term\n\n\n# Vectorize the erf_rapid function\nerfinv_vec = np.vectorize(erfinv)\n\nx = np.linspace(-1, 1, 1000)\ny_rapid = erfinv_vec(x)\ny_scipy = scipy_erfinv(x)\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid)\ny_rapid = y_rapid[mask]\ny_scipy = y_scipy[mask]\n\nmse = np.mean((y_rapid - y_scipy) ** 2)\nprint(f\"MSE: {mse}\")\n\n\n\n\nCode\nprint(f\"MSE: {mse}\")\nprint(f\"Max Error: {np.max(np.abs(y_rapid - y_scipy))}\")\n\n\nMSE: 1.7424258166713035e-07\nMax Error: 0.003953770269546908\n\n\nNow re-peat the experiment but using \\(a=0.140012\\)\n\n\nCode\ny_rapid2 = erfinv_vec(x, a=0.140012)\ny_scipy = scipy_erfinv(x)\n# compute mask where y_rapid isn't infinite\nmask = np.isfinite(y_rapid2)\ny_rapid2 = y_rapid2[mask]\ny_scipy = y_scipy[mask]\n\n\n\n\nCode\nmse = np.mean((y_rapid2 - y_scipy) ** 2)\nprint(f\"MSE: {mse}\")\nprint(f\"Max Error: {np.max(np.abs(y_rapid2 - y_scipy))}\")\n\n\nMSE: 8.462108415211391e-07\nMax Error: 0.007140781441225208"
  },
  {
    "objectID": "posts/pytorch_erfinv/index.html#pytorch-implementation",
    "href": "posts/pytorch_erfinv/index.html#pytorch-implementation",
    "title": "The Inverse Error Function",
    "section": "",
    "text": "Now that we have a decent approximation of the inverse error function, we can implement it in pytorch. I will be using the \\(a=0.147\\) approximation method since it gives a less maximum error.\n[TO DO: Add pytorch implementation]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "taiphamd",
    "section": "",
    "text": "The Inverse Error Function\n\n\n\n\n\n\n\npytorch\n\n\nmetal api\n\n\nmacos\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\nPeter Pham\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Peter, a software engineer based in Portland, OR."
  },
  {
    "objectID": "pr.html",
    "href": "pr.html",
    "title": "Open Source",
    "section": "",
    "text": "Contributions\n\nfixing a bug in ONNX reduction ops during export"
  }
]